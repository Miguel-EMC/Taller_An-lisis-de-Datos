{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'C:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\backports\\\\__init__.py'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting facebook-scraper\n",
      "  Using cached facebook_scraper-0.2.53-py3-none-any.whl (41 kB)\n",
      "Collecting dateparser<2.0.0,>=1.0.0\n",
      "  Using cached dateparser-1.1.0-py2.py3-none-any.whl (288 kB)\n",
      "Collecting demjson3<4.0.0,>=3.0.5\n",
      "  Using cached demjson3-3.0.5-py3-none-any.whl\n",
      "Collecting requests-html<0.11.0,>=0.10.0\n",
      "  Using cached requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pytz in c:\\programdata\\anaconda3\\lib\\site-packages (from dateparser<2.0.0,>=1.0.0->facebook-scraper) (2021.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from dateparser<2.0.0,>=1.0.0->facebook-scraper) (2.8.1)\n",
      "Collecting tzlocal\n",
      "  Using cached tzlocal-4.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from dateparser<2.0.0,>=1.0.0->facebook-scraper) (2021.4.4)\n",
      "Collecting parse\n",
      "  Using cached parse-1.19.0-py3-none-any.whl\n",
      "Collecting pyquery\n",
      "  Using cached pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.25.1)\n",
      "Collecting w3lib\n",
      "  Using cached w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pyppeteer>=0.0.14\n",
      "  Using cached pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: bs4 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (0.0.1)\n",
      "Collecting fake-useragent\n",
      "  Using cached fake_useragent-0.1.11-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.59.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook-scraper) (3.10.0)\n",
      "Collecting certifi>=2021\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting websockets<11.0,>=10.0\n",
      "  Using cached websockets-10.1-cp38-cp38-win_amd64.whl (97 kB)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook-scraper) (1.4.4)\n",
      "Collecting pyee<9.0.0,>=8.1.0\n",
      "  Using cached pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook-scraper) (1.26.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook-scraper) (3.4.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.2.1)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.6.3)\n",
      "Collecting cssselect>0.7.9\n",
      "  Using cached cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil->dateparser<2.0.0,>=1.0.0->facebook-scraper) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.10)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting backports.zoneinfo\n",
      "  Using cached backports.zoneinfo-0.2.1-cp38-cp38-win_amd64.whl (38 kB)\n",
      "Requirement already satisfied: tzdata in c:\\programdata\\anaconda3\\lib\\site-packages (from tzlocal->dateparser<2.0.0,>=1.0.0->facebook-scraper) (2021.5)\n",
      "Installing collected packages: backports.zoneinfo, websockets, pytz-deprecation-shim, pyee, cssselect, certifi, w3lib, tzlocal, pyquery, pyppeteer, parse, fake-useragent, requests-html, demjson3, dateparser, facebook-scraper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install facebook-scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts\n",
    "import couchdb\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "couch=couchdb.Server('http://kevin:Kevin1994@127.0.0.1:5984')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombredb='nike'\n",
    "db=couch[nombredb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "guardado exitosamente\n",
      "2\n",
      "guardado exitosamente\n",
      "3\n",
      "guardado exitosamente\n",
      "4\n",
      "guardado exitosamente\n",
      "5\n",
      "guardado exitosamente\n",
      "6\n",
      "guardado exitosamente\n",
      "7\n",
      "guardado exitosamente\n",
      "8\n",
      "guardado exitosamente\n",
      "9\n",
      "guardado exitosamente\n",
      "10\n",
      "guardado exitosamente\n",
      "11\n",
      "guardado exitosamente\n",
      "12\n",
      "guardado exitosamente\n",
      "13\n",
      "guardado exitosamente\n",
      "14\n",
      "guardado exitosamente\n",
      "15\n",
      "guardado exitosamente\n",
      "16\n",
      "guardado exitosamente\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for post in get_posts('Nike', pages=50, extra_info=True):\n",
    "    print(i)\n",
    "    i=i+1\n",
    "    time.sleep(5)\n",
    "    \n",
    "    id=post['post_id']\n",
    "    doc={}\n",
    "     \n",
    "    doc['id']=id\n",
    "    \n",
    "    mydate=post['time']\n",
    "    \n",
    "    try:\n",
    "        doc['texto']=post['text']\n",
    "        doc['date']=mydate.timestamp()\n",
    "        doc['likes']=post['likes']\n",
    "        doc['comments']=post['comments']\n",
    "        doc['shares']=post['shares']\n",
    "        try:\n",
    "            doc['reactions']=post['reactions']\n",
    "        except:\n",
    "            doc['reactions']={}\n",
    "\n",
    "        doc['post_url']=post['post_url']\n",
    "        db.save(doc)\n",
    "\n",
    "    \n",
    "        print(\"guardado exitosamente\")\n",
    "\n",
    "    except Exception as e:    \n",
    "        print(\"no se pudo grabar:\" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
